{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      " Counter({'idle': 2715, 'space': 917, 'e': 497, 'BackSpace': 429, 'i': 328, 'a': 320, 'o': 302, 't': 289, 'r': 250, 'n': 246, 's': 215, 'u': 184, 'l': 183, 'h': 162, 'd': 159, 'c': 155, 'y': 119, 'g': 109, 'm': 109, 'p': 108, 'w': 103, 'b': 91, 'k': 86, 'f': 85, 'dot': 84, 'v': 73, 'comma': 66, 'j': 62, 'z': 58, 'x': 54, 'q': 52})\n",
      "Val:\n",
      " Counter({'idle': 973, 'space': 311, 'e': 162, 'BackSpace': 136, 'i': 112, 'a': 108, 't': 96, 'o': 87, 'n': 75, 'r': 73, 'h': 67, 's': 62, 'u': 57, 'l': 52, 'c': 49, 'd': 49, 'f': 43, 'y': 41, 'm': 39, 'g': 38, 'w': 28, 'p': 26, 'comma': 26, 'b': 26, 'z': 24, 'dot': 23, 'v': 22, 'x': 18, 'k': 16, 'j': 16, 'q': 15})\n",
      "Test:\n",
      " Counter({'idle': 944, 'space': 316, 'BackSpace': 164, 'e': 147, 'i': 112, 't': 103, 'o': 94, 'n': 85, 'r': 84, 'a': 80, 's': 64, 'l': 62, 'c': 57, 'u': 54, 'm': 53, 'd': 46, 'h': 42, 'w': 37, 'f': 36, 'y': 36, 'g': 33, 'p': 31, 'b': 31, 'dot': 28, 'k': 23, 'v': 23, 'q': 21, 'comma': 20, 'x': 20, 'z': 13, 'j': 11})\n",
      "train_weights: \n",
      " [4.2082111436950145, 3.306451612903226, 0.6474170990300022, 0.10229905542684013, 0.3028810637773947, 0.8679435483870968, 3.0521091811414394, 1.7918834547346514, 1.7468046256847232, 0.558836892321672, 3.267552182163188, 2.548091151228174, 1.7144563918757467, 0.8467741935483871, 4.479708636836628, 3.229557389347337, 1.5177154944473823, 2.548091151228174, 1.1290322580645162, 0.9196752830591753, 2.57168458781362, 5.341191066997519, 1.110967741935484, 1.2918229557389347, 0.9610447594597611, 1.5094670406732118, 3.8046840477242596, 2.696523645474475, 5.14336917562724, 2.333965844402277, 4.788654060066741]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized because the shapes did not match:\n",
      "- vivit.embeddings.position_embeddings: found shape torch.Size([1, 3137, 768]) in the checkpoint and torch.Size([1, 785, 768]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([31, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([31]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type                        | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | loss_fn   | CrossEntropyLoss            | 0      | train\n",
      "1 | train_acc | MulticlassAccuracy          | 0      | train\n",
      "2 | val_acc   | MulticlassAccuracy          | 0      | train\n",
      "3 | test_acc  | MulticlassAccuracy          | 0      | train\n",
      "4 | model     | VivitForVideoClassification | 86.9 M | eval \n",
      "------------------------------------------------------------------\n",
      "86.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "86.9 M    Total params\n",
      "347.456   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/2153 [00:10<6:00:53,  0.10it/s, v_num=12]"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from key_utils import KeySegmentDataModule, KeyClf, id2label, label2id\n",
    "except:\n",
    "    import sys\n",
    "    sys.path.append(\"/kaggle/input/keystroke-util\")\n",
    "    from key_utils import KeySegmentDataModule, KeyClf, id2label, label2id\n",
    "\n",
    "from transformers import VivitImageProcessor, VivitForVideoClassification\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "import torchvision\n",
    "import torchvision.transforms.functional\n",
    "import lightning as L\n",
    "import torch\n",
    "\n",
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "\n",
    "def preprocess(frames): \n",
    "    out = image_processor(list(frames), return_tensors=\"pt\")\n",
    "    pixel_values = out['pixel_values'][0]\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def transforms(frames):\n",
    "    h, w = frames.shape[-2], frames.shape[-1]\n",
    "\n",
    "    frames = torchvision.transforms.functional.resized_crop(\n",
    "                              frames, \n",
    "                              top=h//2, \n",
    "                              left=0, \n",
    "                              height=h//2,\n",
    "                              width=w,\n",
    "                              size=(224, 224))\n",
    "    \n",
    "    out = image_processor(list(frames), return_tensors=\"pt\")\n",
    "    pixel_values = out['pixel_values'][0]\n",
    "    return pixel_values\n",
    "    \n",
    "dm = KeySegmentDataModule(segment_dir='datasets/angle/segments_dir', \n",
    "                          num_workers=0,\n",
    "                          transforms=transforms)\n",
    "weights = dm.train_weights\n",
    "\n",
    "\n",
    "class ViVitKeyClf(KeyClf):\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        super().__init__(weights, learning_rate)\n",
    "        self.model = VivitForVideoClassification.from_pretrained(\n",
    "            \"google/vivit-b-16x2-kinetics400\", \n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True,\n",
    "            num_frames=8,\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        videos, targets = batch\n",
    "        out = self.model(videos)\n",
    "        preds = out.logits\n",
    "        loss = self.loss_fn(preds, targets.long())\n",
    "        pred_ids = torch.argmax(preds, dim=1)\n",
    "        return loss, pred_ids\n",
    "\n",
    "\n",
    "\n",
    "module = ViVitKeyClf()\n",
    "trainer = L.Trainer(\n",
    "    # deterministic=True,\n",
    "    # devices=[0, 1],\n",
    "    # accelerator=\"gpu\",\n",
    "    fast_dev_run=False,\n",
    "    max_epochs=100,\n",
    "    callbacks=EarlyStopping(monitor='val_loss', patience=5),\n",
    ")\n",
    "\n",
    "trainer.fit(module, dm)\n",
    "# trainer.test(module, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
