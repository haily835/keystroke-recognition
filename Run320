{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8815229,"sourceType":"datasetVersion","datasetId":5297798}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch lightning torchvision pyav","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\nimport pathlib\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.io import read_video\nimport lightning as L\nfrom lightning.pytorch.loggers import CSVLogger\nimport torchmetrics\nfrom lightning.pytorch.callbacks import EarlyStopping\nfrom sklearn.metrics import classification_report, confusion_matrix, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\nLOCAL = False\n\n# Training hyperparameters\nIMG_SIZE = 320\nFRAMES_PER_VIDEO = 8\nNUM_CLASSES = 30\nLEARNING_RATE = 0.001\nBATCH_SIZE = 16\nMAX_EPOCHS = 10000\nMAX_TIME = \"00:11:00:00\"\n\n# Datasets\nLOCAL_DATA_DIR = f\"./datasets/key_clf_data_{IMG_SIZE}_{IMG_SIZE}\"\nKAGGLE_DATA_DIR = f\"/kaggle/input/key-clf/key_clf_data_{IMG_SIZE}_{IMG_SIZE}/key_clf_data_{IMG_SIZE}_{IMG_SIZE}\"\n\nNUM_WORKERS = 4\n\nFAST_DEV_RUN = False\nCHECKPOINT_DIR = \"resnet/\"\n\n# Compute related\nACCELERATOR = \"gpu\"\nDEVICES = [0,1]\n\nid2Label = ['BackSpace', 'Comma', 'Space', 'Stop', \n            'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n            'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', \n            'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n            'y', 'z']\n\n#### RESNET 3D #### \ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4) ).zero_()\n    \n    if isinstance(out.data, torch.cuda.FloatStorage): zero_pads = zero_pads.cuda()\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n    return out\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n\n        self.conv2 = nn.Conv3d(\n            planes, planes, \n            kernel_size=3, stride=stride, padding=1, \n            bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        \n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * 4)\n        \n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, sample_size, sample_duration, shortcut_type='B', num_classes=400):\n        \"\"\"\n        block: basic block or bottle neck\n        layers: define Resnet architecture 34, 101, 152 etc\n        sample size: image size\n        shortcut_type: 'A' or 'B'\n        num_classes: ...\n        \"\"\"\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n        self.layer2 = self._make_layer(block, 128, layers[1], shortcut_type, stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], shortcut_type, stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], shortcut_type, stride=2)\n        \n        \n        last_duration = int(math.ceil(sample_duration / 16))\n        last_size = int(math.ceil(sample_size / 32))\n        self.avgpool = nn.AvgPool3d(\n            (last_duration, last_size, last_size), stride=1)\n        \n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == 'A':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(self.inplanes,planes * block.expansion,kernel_size=1,stride=stride,bias=False), \n                    nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\ndef resnet10(**kwargs): return ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\ndef resnet18(**kwargs): return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\ndef resnet34(**kwargs): return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\ndef resnet50(**kwargs): return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\ndef resnet101(**kwargs): return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\ndef resnet152(**kwargs): return ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\ndef resnet200(**kwargs): return ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)\n\n\n####### \nclass KeyClf(L.LightningModule):\n    def __init__(self, img_size, frames_per_video, num_classes, learning_rate, weights):\n        super().__init__()\n        self.model = resnet101(sample_size=img_size, sample_duration=frames_per_video,\n                               shortcut_type='B', num_classes=num_classes)\n        \n        self.loss_fn = torch.nn.CrossEntropyLoss(torch.tensor(weights))\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.lr = learning_rate\n        self.test_y = []\n        self.test_pred = []\n\n    def test_step(self, batch):\n        videos, targets = batch\n        preds = self.model(videos)\n        self.test_pred.append(preds)\n        self.test_y.append(targets)\n\n        loss = self.loss_fn(preds, targets.long())\n        test_acc = self.accuracy(preds, targets)\n        self.log_dict({'test_acc': test_acc, 'test_loss': loss})\n\n\n    def on_test_end(self) -> None:\n        preds = torch.cat(self.test_pred)\n        targets = torch.cat(self.test_y)\n        acc = self.accuracy(preds, targets)\n        print('acc: ', acc)\n#         print(\"target\", targets[:5])\n#         print(\"preds\", torch.argmax(preds[:5], 1))\n#         targets = targets.cpu().numpy()\n#         preds = torch.argmax(preds, 1).cpu().numpy()\n#         print(classification_report(targets, preds))\n#         cm = confusion_matrix(targets, preds)\n#         plt.figure(figsize=(16, 12))\n#         sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=id2Label, yticklabels=id2Label)\n#         plt.xlabel('Predicted labels')\n#         plt.ylabel('True labels')\n#         plt.title('Confusion Matrix')\n#         plt.savefig('result.png')\n\n    \n    def training_step(self, batch):\n        videos, targets = batch\n        preds = self.model(videos)\n        loss = self.loss_fn(preds, targets.long())\n        self.log_dict({ \"train_loss\": loss, \"train_acc\": self.accuracy(preds, targets)}, \n                      on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch):\n        videos, targets = batch\n        preds = self.model(videos)\n        loss = self.loss_fn(preds, targets.long())\n        self.log_dict({ \"val_loss\": loss, \"val_acc\": self.accuracy(preds, targets)}, \n                      on_step=False, on_epoch=True, prog_bar=True,)\n        return loss\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.model.parameters(), lr=self.lr)\n    \nclass KeyClsDataset(Dataset):\n    def __init__(self, data_dir, mode):\n        self.dataset_root_path = pathlib.Path(data_dir)\n        self.all_video_file_paths =  list(self.dataset_root_path.glob(f\"{mode}/*/*.mp4\"))\n        self.class_labels = sorted({str(path).split(\"/\")[-2] for path in self.all_video_file_paths})\n        self.label2id = {label: i for i, label in enumerate(self.class_labels)}\n        self.id2label  = {i: label for label, i in self.label2id.items()}\n  \n    def __len__(self):\n        return len(self.all_video_file_paths)\n\n    def __getitem__(self, idx):\n        file_path = self.all_video_file_paths[idx]\n        vframes, _, _ = read_video(file_path, pts_unit='sec')\n        label = str(file_path).split(\"/\")[-2]\n        # permute to (num_frames, num_channels, height, width)\n        vframes = vframes.permute(3, 0, 1, 2).float() / 255.0\n        return vframes, self.label2id[label]\n\nclass KeyClsDataModule(L.LightningDataModule):\n    def __init__(self, batch_size, data_dir):\n        super().__init__()\n        self.batch_size = batch_size\n        self.dataset_root_path = pathlib.Path(data_dir)\n        self.data_dir = data_dir\n        weights = {}\n        probs = {'Letter': id2Label}\n        \n        for split in ['train', 'val', 'test']:\n            video_paths =  list(self.dataset_root_path.glob(f\"{split}/*/*.mp4\"))\n\n            class_labels = sorted({str(path).split(\"/\")[-2] for path in video_paths})\n\n            total = len(video_paths)\n            weights[split] = []\n            probs[split] = []\n            for label in class_labels:\n                samples = len(list(self.dataset_root_path.glob(f\"{split}/{label}/*.mp4\")))\n                probs[split].append(samples)\n                weights[split].append(total / (NUM_CLASSES * samples))\n            \n        self.weights = weights\n        self.probs = probs\n   \n    def train_dataloader(self):\n        train_dataset = KeyClsDataset(self.data_dir, 'train')\n        print(\"Train dataset:\", len(train_dataset))\n        return DataLoader(train_dataset, batch_size=self.batch_size, num_workers=NUM_WORKERS, persistent_workers=True)\n    \n    def val_dataloader(self):\n        val_dataset = KeyClsDataset(self.data_dir, 'val')\n        print(\"Val dataset:\", len(val_dataset))\n        return DataLoader(val_dataset, batch_size=self.batch_size, num_workers=NUM_WORKERS, persistent_workers=True)\n    \n    def test_dataloader(self):\n        test_dataset = KeyClsDataset(self.data_dir, 'test')\n        print(\"Test dataset:\", len(test_dataset))\n        return DataLoader(test_dataset, batch_size=self.batch_size, num_workers=NUM_WORKERS, persistent_workers=True)\n\n\nif __name__ == '__main__':\n    dm = KeyClsDataModule(batch_size=BATCH_SIZE, \n                          data_dir=LOCAL_DATA_DIR if LOCAL else KAGGLE_DATA_DIR)\n    print(pd.DataFrame(dm.probs))\n    model = KeyClf(img_size=IMG_SIZE, frames_per_video=FRAMES_PER_VIDEO, \n                   num_classes=NUM_CLASSES, learning_rate=LEARNING_RATE, \n                   weights = dm.weights['train'])\n    \n    logger = CSVLogger(\"logs\", name=f\"resnet_101_img_{IMG_SIZE}\", flush_logs_every_n_steps=1)\n    \n    if LOCAL:\n        trainer = L.Trainer(\n                    max_time=MAX_TIME,\n                    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=10)],\n                    fast_dev_run=True,\n                    logger=logger,\n                    accelerator=\"cpu\"\n                )\n    else: \n        trainer = L.Trainer(\n                devices=DEVICES,\n                max_time=MAX_TIME,\n                callbacks=[EarlyStopping(monitor=\"val_loss\", patience=10)],\n                fast_dev_run=FAST_DEV_RUN,\n                logger=logger,\n                accelerator=ACCELERATOR\n            )\n    \n    trainer.fit(model, dm)\n    trainer.test(model, dm)\n\n    \n       ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-29T04:39:55.522216Z","iopub.execute_input":"2024-06-29T04:39:55.522626Z","iopub.status.idle":"2024-06-29T04:40:25.131827Z","shell.execute_reply.started":"2024-06-29T04:39:55.522597Z","shell.execute_reply":"2024-06-29T04:40:25.130680Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"       Letter  train   val  test\n0   BackSpace   4141   648   400\n1       Comma    452   154    27\n2       Space   6703  1179   618\n3        Stop    304   131    44\n4           a   2929   372   287\n5           b    568   140    26\n6           c   1208   173   133\n7           d   1163   191   107\n8           e   4370   485   425\n9           f    648   143    58\n10          g    691   163    78\n11          h   1161   175    90\n12          i   2809   395   301\n13          j    345   129     8\n14          k    294   149    25\n15          l   1592   193   121\n16          m    827   156   103\n17          n   2360   228   254\n18          o   2498   296   258\n19          p    908   158    92\n20          q    279   137     5\n21          r   2256   264   247\n22          s   1837   215   192\n23          t   2834   232   314\n24          u   1408   246   117\n25          v    375   129    44\n26          w    524   145    40\n27          x    431   141     5\n28          y    733   175    67\n29          z    388   141     3\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_1686/2064092609.py:171: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\nINFO: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\nINFO: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\nINFO: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 2 processes\n----------------------------------------------------------------------------------------------------\n\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name     | Type               | Params | Mode \n--------------------------------------------------------\n0 | model    | ResNet             | 85.3 M | train\n1 | loss_fn  | CrossEntropyLoss   | 0      | train\n2 | accuracy | MulticlassAccuracy | 0      | train\n--------------------------------------------------------\n85.3 M    Trainable params\n0         Non-trainable params\n85.3 M    Total params\n341.226   Total estimated model params size (MB)\n","output_type":"stream"},{"name":"stdout","text":"Train dataset: 47036Train dataset:\n 47036\nVal dataset: Val dataset:7483 7483\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b4bd45aa39a4edc9b25cd2e2365b422"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"195acaaac55a48baae44141dbfa66349"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\nINFO: `Trainer.fit` stopped: `max_steps=1` reached.\nINFO: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\nINFO: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\nINFO: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 2 processes\n----------------------------------------------------------------------------------------------------\n\nINFO: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n","output_type":"stream"},{"name":"stdout","text":"Test dataset:Test dataset:  44894489\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:215: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a543df4bbe44b3b0f194b99dd563fd"}},"metadata":{}},{"name":"stdout","text":"acc:  ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","output_type":"stream"},{"name":"stdout","text":"tensor(0., device='cuda:1')\nacc:  tensor(0., device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    4.921396732330322    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     4.921396732330322     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}}]}]}