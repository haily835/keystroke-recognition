{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8808384,"sourceType":"datasetVersion","datasetId":5297798}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch lightning torchvision pyav","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\nimport pathlib\nimport torch\n# from GesRec.models.resnet import resnet101\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.io import read_video\nimport lightning as L\nfrom lightning.pytorch.loggers import CSVLogger\nimport torchmetrics\nfrom lightning.pytorch.callbacks import EarlyStopping\n\n# Training hyperparameters\nIMG_SIZE = 112\nFRAMES_PER_VIDEO = 8\nNUM_CLASSES = 30\nLEARNING_RATE = 0.001\nBATCH_SIZE = 16\nMAX_EPOCHS = 1000\nMAX_TIME = \"00:10:00:00\"\n\n# Dataset\nDATA_DIR = \"/kaggle/input/key-clf/key_clf_data_112_112\"\nNUM_WORKERS = 4\n\nFAST_DEV_RUN = False\nCHECKPOINT_DIR = \"resnet/\"\n\n# Compute related\nACCELERATOR = \"gpu\"\nDEVICES = [0,1]\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(\n        out.size(0), \n        planes - out.size(1), \n        out.size(2), \n        out.size(3),\n        out.size(4)\n    ).zero_()\n    \n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n\n        self.conv2 = nn.Conv3d(\n            planes, planes, \n            kernel_size=3, stride=stride, padding=1, \n            bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        \n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * 4)\n        \n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self,\n                 block,\n                 layers,\n                 sample_size,\n                 sample_duration,\n                 shortcut_type='B',\n                 num_classes=400):\n        \n        \"\"\"\n        block: basic block or bottle neck\n        layers: define Resnet architecture 34, 101, 152 etc\n        sample size: image size\n        shortcut_type: 'A' or 'B'\n        num_classes: ...\n        \"\"\"\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv3d(\n            3,\n            64,\n            kernel_size=7,\n            stride=(1, 2, 2),\n            padding=(3, 3, 3),\n            bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n\n\n        self.layer1 = self._make_layer(\n            block, 64, layers[0], shortcut_type)\n        self.layer2 = self._make_layer(\n            block, 128, layers[1], shortcut_type, stride=2)\n        self.layer3 = self._make_layer(\n            block, 256, layers[2], shortcut_type, stride=2)\n        self.layer4 = self._make_layer(\n            block, 512, layers[3], shortcut_type, stride=2)\n        \n        \n        last_duration = int(math.ceil(sample_duration / 16))\n        last_size = int(math.ceil(sample_size / 32))\n        self.avgpool = nn.AvgPool3d(\n            (last_duration, last_size, last_size), stride=1)\n        \n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == 'A':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(\n                        self.inplanes,\n                        planes * block.expansion,\n                        kernel_size=1,\n                        stride=stride,\n                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef get_fine_tuning_parameters(model, ft_portion):\n    if ft_portion == \"complete\":\n        return model.parameters()\n\n    elif ft_portion == \"last_layer\":\n        ft_module_names = []\n        ft_module_names.append('classifier')\n\n        parameters = []\n        for k, v in model.named_parameters():\n            for ft_module in ft_module_names:\n                if ft_module in k:\n                    parameters.append({'params': v})\n                    break\n            else:\n                parameters.append({'params': v, 'lr': 0.0})\n        return parameters\n    else:\n        raise ValueError(\"Unsupported ft_portion: 'complete' or 'last_layer' expected\")\n\n\ndef resnet10(**kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n    \"\"\"\n    model = ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\n    return model\n\n\ndef resnet18(**kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\n\ndef resnet34(**kwargs):\n    \"\"\"Constructs a ResNet-34 model.\n    \"\"\"\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnet50(**kwargs):\n    \"\"\"Constructs a ResNet-50 model.\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnet101(**kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\n\ndef resnet152(**kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n\n\ndef resnet200(**kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)\n    return model\n\n\n\nclass KeyClf(L.LightningModule):\n    def __init__(self, img_size, frames_per_video, num_classes, learning_rate, weights):\n        super().__init__()\n        self.model = resnet101(sample_size=img_size,\n                 sample_duration=frames_per_video,\n                 shortcut_type='A',\n                 num_classes=num_classes\n                )\n        \n        self.loss_fn = torch.nn.CrossEntropyLoss(torch.tensor(weights))\n        self.accuracy = torchmetrics.Accuracy(\n            task=\"multiclass\", num_classes=num_classes\n        )\n\n        # self.training_step_preds = []\n        # self.training_step_targets = []\n        # self.validate_step_preds = []\n        # self.validate_step_targets = []\n\n        self.lr = learning_rate\n\n\n    def training_step(self, batch):\n        videos, targets = batch\n        preds = self.model(videos.to('cuda'))\n        loss = self.loss_fn(preds, targets.long())\n        # self.training_step_preds.append(preds)\n        # self.training_step_targets.append(targets)\n\n        self.log_dict(\n            {\n                \"train_loss\": loss,\n                \"train_acc\": self.accuracy(preds, targets),\n            },\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n        )\n\n        return loss\n    \n    # def on_training_epoch_end(self):\n    #     preds = torch.cat(self.training_step_preds)\n    #     targets = torch.cat(self.training_step_targets)\n  \n    #     self.log_dict(\n    #         {\n    #             \"train_acc\": self.accuracy(preds, targets),\n    #         },\n    #         on_step=False,\n    #         on_epoch=True,\n    #         prog_bar=True,\n    #         logger=True,\n    #     )\n\n    #     self.training_step_preds.clear()\n    #     self.training_step_targets.clear()\n\n\n    def validation_step(self, batch):\n        videos, targets = batch\n        preds = self.model(videos)\n        loss = self.loss_fn(preds, targets.long())\n        # self.validate_step_preds.append(preds)\n        # self.validate_step_targets.append(targets)\n\n        self.log_dict(\n            {\n                \"val_loss\": loss,\n                \"val_acc\": self.accuracy(preds, targets),\n            },\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n\n        return loss\n    \n    \n    # def on_validation_epoch_end(self):\n    #     preds = torch.cat(self.validate_step_preds)\n    #     targets = torch.cat(self.validate_step_targets)\n       \n    #     self.log_dict(\n    #         {\n    #             \"val_acc\": self.accuracy(preds, targets),\n    #         },\n    #         on_step=False,\n    #         on_epoch=True,\n    #         prog_bar=True,\n    #         logger=True\n    #     )\n\n    #     self.validate_step_preds.clear()\n    #     self.validate_step_targets.clear()\n    \n        \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.model.parameters(), lr=self.lr)\n    \nclass KeyStrokeClsDataset(Dataset):\n    def __init__(self, data_dir, mode):\n        self.dataset_root_path = pathlib.Path(data_dir)\n        self.all_video_file_paths =  list(self.dataset_root_path.glob(f\"{mode}/*/*.mp4\"))\n\n        self.class_labels = sorted({str(path).split(\"/\")[-2] for path in self.all_video_file_paths})\n        \n        self.label2id = {label: i for i, label in enumerate(self.class_labels)}\n        self.id2label  = {i: label for label, i in self.label2id.items()}\n  \n    def __len__(self):\n        return len(self.all_video_file_paths)\n\n    def __getitem__(self, idx):\n        file_path = self.all_video_file_paths[idx]\n        vframes, _, _ = read_video(file_path, pts_unit='sec')\n        label = str(file_path).split(\"/\")[-2]\n\n        # permute to (num_frames, num_channels, height, width)\n        vframes = vframes.permute(3, 0, 1, 2).float() / 255.0\n    \n        return vframes, self.label2id[label]\n\n\nclass KeyClsData(L.LightningDataModule):\n    def __init__(self, batch_size, data_dir):\n        super().__init__()\n        self.batch_size = batch_size\n        self.dataset_root_path = pathlib.Path(data_dir)\n        self.data_dir = data_dir\n\n        train_video_file_paths =  list(self.dataset_root_path.glob(f\"train/*/*.mp4\"))\n\n        class_labels = sorted({str(path).split(\"/\")[-2] for path in train_video_file_paths})\n        print('class_labels: ', class_labels)\n        \n        total = len(train_video_file_paths)\n        weights = []\n        for label in class_labels:\n            samples = len(list(self.dataset_root_path.glob(f\"train/{label}/*.mp4\")))\n            weights.append(total / (NUM_CLASSES * samples))\n        self.weights = weights\n        print('class_weights: ', weights)\n   \n        \n    def train_dataloader(self):\n        train_dataset = KeyStrokeClsDataset(self.data_dir, 'train')\n        print(\"Train dataset:\", len(train_dataset))\n        return DataLoader(train_dataset, batch_size=self.batch_size, num_workers=NUM_WORKERS, persistent_workers=True)\n    \n    def val_dataloader(self):\n        val_dataset = KeyStrokeClsDataset(self.data_dir, 'val')\n        print(\"Val dataset:\", len(val_dataset))\n        return DataLoader(val_dataset, batch_size=self.batch_size, num_workers=NUM_WORKERS, persistent_workers=True)\n    \n    def test_dataloader(self):\n        test_dataset = KeyStrokeClsDataset(self.data_dir, 'test')\n        print(\"Test dataset:\", len(test_dataset))\n        return DataLoader(test_dataset, batch_size=self.batch_size, num_workers=NUM_WORKERS, persistent_workers=True)\n    \n\nif __name__ == '__main__':\n    data = KeyClsData(batch_size=BATCH_SIZE, data_dir=DATA_DIR)\n    model = KeyClf(img_size=IMG_SIZE, \n                frames_per_video=FRAMES_PER_VIDEO,\n                num_classes=NUM_CLASSES, \n                learning_rate=LEARNING_RATE,\n                weights = data.weights)\n\n    logger = CSVLogger(\"logs\", name=f\"resnet\", flush_logs_every_n_steps=1)\n    trainer = L.Trainer(\n        #deterministic=True,\n        devices=DEVICES,\n        max_time=MAX_TIME,\n        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n        default_root_dir=CHECKPOINT_DIR,\n        fast_dev_run=FAST_DEV_RUN,\n        logger=logger,\n        accelerator=ACCELERATOR\n    )\n    trainer.fit(model, data)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T07:19:26.276986Z","iopub.execute_input":"2024-06-28T07:19:26.277373Z","iopub.status.idle":"2024-06-28T07:21:45.080261Z","shell.execute_reply.started":"2024-06-28T07:19:26.277343Z","shell.execute_reply":"2024-06-28T07:21:45.078813Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"class_labels:  ['BackSpace', 'Comma', 'Space', 'Stop', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nclass_weights:  [0.3786203010544957, 3.4687315634218288, 0.23390521656969515, 5.157456140350877, 0.535290770456356, 2.7603286384976524, 1.2979028697571744, 1.3481226712525078, 0.35877955758962626, 2.4195473251028807, 2.2689821514712976, 1.350445018662073, 0.5581583007001305, 4.544541062801932, 5.332879818594105, 0.9848408710217755, 1.8958484482063684, 0.6643502824858757, 0.6276487856952229, 1.7267254038179147, 5.619593787335723, 0.6949763593380615, 0.8534930139720559, 0.5532345330510469, 1.1135416666666667, 4.180977777777778, 2.9921119592875316, 3.637741686001547, 2.1389722601182357, 4.040893470790378]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_504/1399779437.py:192: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\nINFO: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\nINFO: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\nINFO: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 2 processes\n----------------------------------------------------------------------------------------------------\n\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name     | Type               | Params | Mode \n--------------------------------------------------------\n0 | model    | ResNet             | 82.5 M | train\n1 | loss_fn  | CrossEntropyLoss   | 0      | train\n2 | accuracy | MulticlassAccuracy | 0      | train\n--------------------------------------------------------\n82.5 M    Trainable params\n0         Non-trainable params\n82.5 M    Total params\n330.120   Total estimated model params size (MB)\n","output_type":"stream"},{"name":"stdout","text":"Train dataset: 47036\nTrain dataset: 47036\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n","output_type":"stream"}]}]}