{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8874695,"sourceType":"datasetVersion","datasetId":5342092},{"sourceId":8880876,"sourceType":"datasetVersion","datasetId":5344689},{"sourceId":74816,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":62837}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightning torchvision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport lightning as L\nimport pandas as pd\nimport torchvision\nimport numpy as np\nimport torch.nn.functional as F \nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\nimport pathlib\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.io import read_video\nimport lightning as L\nfrom lightning.pytorch.loggers import CSVLogger\nimport torchmetrics\nfrom lightning.pytorch.callbacks import EarlyStopping\nfrom sklearn.metrics import classification_report, confusion_matrix, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom torchvision.transforms import CenterCrop, v2\nfrom datetime import datetime\nimport csv\nimport glob\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:13:43.663466Z","iopub.execute_input":"2024-07-07T19:13:43.664201Z","iopub.status.idle":"2024-07-07T19:13:43.672172Z","shell.execute_reply.started":"2024-07-07T19:13:43.664152Z","shell.execute_reply":"2024-07-07T19:13:43.671185Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"id2Label = ['[i]', 'BackSpace', ',', '[s]', '.', \n            'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n            'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', \n            'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n            'y', 'z']\nlabel2Id  = {label: i for i, label in enumerate(id2Label)}\n\nNUM_WORKERS = 4\nf_after = 2 # number of frames after\nf_before = 2 # number of frames before\ngap = 2 # gap between idle video segment and non-idle video segment\ntotal_window = f_after + f_before + 1","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:13:48.853780Z","iopub.execute_input":"2024-07-07T19:13:48.854187Z","iopub.status.idle":"2024-07-07T19:13:48.860778Z","shell.execute_reply.started":"2024-07-07T19:13:48.854157Z","shell.execute_reply":"2024-07-07T19:13:48.859645Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Resnet","metadata":{}},{"cell_type":"code","source":"#### RESNET 3D #### \ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4) ).zero_()\n    \n    if isinstance(out.data, torch.cuda.FloatStorage): zero_pads = zero_pads.cuda()\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n    return out\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n\n        self.conv2 = nn.Conv3d(\n            planes, planes, \n            kernel_size=3, stride=stride, padding=1, \n            bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        \n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * 4)\n        \n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, sample_size, sample_duration, shortcut_type='B', num_classes=400):\n        \"\"\"\n        block: basic block or bottle neck\n        layers: define Resnet architecture 34, 101, 152 etc\n        sample size: image size\n        shortcut_type: 'A' or 'B'\n        num_classes: ...\n        \"\"\"\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n        self.layer2 = self._make_layer(block, 128, layers[1], shortcut_type, stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], shortcut_type, stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], shortcut_type, stride=2)\n        \n        \n        last_duration = int(math.ceil(sample_duration / 16))\n        last_size = int(math.ceil(sample_size / 32))\n        self.avgpool = nn.AvgPool3d(\n            (last_duration, last_size, last_size), stride=1)\n        \n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == 'A':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(self.inplanes,planes * block.expansion,kernel_size=1,stride=stride,bias=False), \n                    nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\ndef resnet10(**kwargs): return ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\ndef resnet18(**kwargs): return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\ndef resnet34(**kwargs): return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\ndef resnet50(**kwargs): return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\ndef resnet101(**kwargs): return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\ndef resnet152(**kwargs): return ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\ndef resnet200(**kwargs): return ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:13:55.176938Z","iopub.execute_input":"2024-07-07T19:13:55.177332Z","iopub.status.idle":"2024-07-07T19:13:55.212799Z","shell.execute_reply.started":"2024-07-07T19:13:55.177300Z","shell.execute_reply":"2024-07-07T19:13:55.211891Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Lightning dataset","metadata":{}},{"cell_type":"code","source":"class KeyDataset(torch.utils.data.Dataset):\n    def __init__(self, video_name, labels_dir, videos_dir):\n        segments = []\n        # Infer idle frames.\n        self.labels_dir = labels_dir\n        self.videos_dir = videos_dir\n        df = pd.read_csv(f'{self.labels_dir}/{video_name}.csv')\n        for index, row in df.iterrows():\n            key_frame = int(row['Frame'])  # Frame number where key was pressed\n            key_value = row['Key']  # Key pressed\n            if key_value not in id2Label:\n                key_value = '[s]'\n            \n            is_idle_before = False\n            if index == 0:\n                pos_start = max(key_frame - f_before, 0)\n                pos_end = key_frame + f_after\n                neg_start = 0\n                neg_end = pos_start - gap\n                is_idle_before = True\n            else:\n                prev_key_frame = df.iloc[index - 1]['Frame']\n                pos_start = max(key_frame - f_before, 0)\n                pos_end = key_frame + f_after\n                prev_pos_end = prev_key_frame + f_after\n                if (pos_start - prev_pos_end) - 1 >= (f_after + f_before + 1 + gap * 2):\n                    neg_start =  prev_pos_end + gap\n                    neg_end = pos_start - gap\n                    is_idle_before = True\n            \n            \n            # Negative class video segments before\n            if is_idle_before:\n                j = neg_start\n                while (j + total_window - 1) <= neg_end:\n                    segments.append(([j, j + total_window - 1], \"[i]\"))\n                    j += total_window\n            \n            # Current video with keystroke\n            segments.append(([pos_start, pos_end], key_value))\n        \n        self.video_name = video_name\n        self.segments = segments\n    def __len__(self):\n        return len(self.segments)\n\n    def __getitem__(self, idx):\n        (start, end), label = self.segments[idx]\n        \n        frames = []\n        for i in range(start, end + 1):\n            image = torchvision.io.read_image(f\"{self.videos_dir}/{self.video_name}/frame_{i}.jpg\")\n            \n            frames.append(image)\n       \n        return torch.stack(frames), label2Id[label]\n    \n    def get_class_counts(self):\n        labels = [segment[1] for segment in self.segments]\n        unique_elements, counts = np.unique(labels, return_counts=True)\n        occurrences = dict(zip(unique_elements, counts))\n        weights = np.zeros(len(id2Label))\n        for label, count in occurrences.items():\n            weights[label2Id[label]] = count\n        return weights\n\nclass KeyDataModule(L.LightningDataModule):\n    def __init__(self, batch_size, labels_dir, videos_dir, train_vids, val_vids, test_vids):\n        super().__init__()\n        self.batch_size = batch_size\n        self.train_datasets = [KeyDataset(video_name, labels_dir, videos_dir) for video_name in train_vids]\n        self.val_datasets = [KeyDataset(video_name, labels_dir, videos_dir) for video_name in val_vids]\n        self.test_datasets = [KeyDataset(video_name, labels_dir, videos_dir) for video_name in test_vids]\n        \n        self.train_dataset = torch.utils.data.ConcatDataset(self.train_datasets)\n        self.test_dataset = torch.utils.data.ConcatDataset(self.test_datasets)\n        self.val_dataset = torch.utils.data.ConcatDataset(self.val_datasets)\n        \n        \n        \n        print(f\"Train: {len(self.train_dataset)}; Val: {len(self.val_dataset)}; Test: {len(self.test_dataset)}\")\n        \n        train_counts = np.array(\n            [d.get_class_counts() for d in self.train_datasets]).sum(axis=0)\n        print(f\"Train counts: {train_counts}\")\n        train_total_samples = np.array([len(d) for d in self.train_datasets]).sum(axis=0)\n        self.train_weights = train_counts / (train_total_samples * len(id2Label))\n                                        \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, \n                          batch_size=self.batch_size, \n                          num_workers=NUM_WORKERS)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, \n                          batch_size=self.batch_size, \n                          num_workers=NUM_WORKERS,\n                          shuffle=False)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, \n                          batch_size=self.batch_size, \n                          num_workers=NUM_WORKERS,\n                          shuffle=False)\n\nclass KeyClf(L.LightningModule):\n    def __init__(self, img_size, num_classes, learning_rate, weights):\n        super().__init__()\n        self.model = resnet34(sample_size=img_size, \n                               sample_duration=total_window,\n                               shortcut_type='B', \n                               num_classes=num_classes)\n        \n        self.loss_fn = torch.nn.CrossEntropyLoss(torch.tensor(weights).float())\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.lr = learning_rate\n        self.transforms = v2.Compose([\n            v2.CenterCrop(img_size),\n            v2.ToDtype(torch.float32, scale=True),\n        ])\n        \n        self.test_preds = []\n        self.test_targets = []\n        self.save_hyperparameters()\n\n\n    def test_step(self, batch):\n        videos, targets = batch\n        videos = self.transforms(videos)\n        videos = videos.permute(0, 2, 1, 3, 4)\n        preds = self.model(videos)\n\n        pred_ids = torch.argmax(self.model(videos), dim=1).squeeze()\n        pred_labels = [id2Label[_id] for _id in pred_ids]\n        self.test_preds += pred_labels\n        self.test_targets += [id2Label[_id] for _id in targets]\n        \n        loss = self.loss_fn(preds, targets.long())\n        self.log('test_loss', loss)\n        self.log('test_acc', self.accuracy(preds, targets))\n    \n    def on_test_end(self):\n        print(classification_report(self.test_targets, self.test_preds))\n        \n    def training_step(self, batch):\n        videos, targets = batch\n        videos = self.transforms(videos)\n        videos = videos.permute(0, 2, 1, 3, 4)\n        preds = self.model(videos)\n        loss = self.loss_fn(preds, targets.long())\n        self.log('train_loss', loss)\n        self.log('train_acc', self.accuracy(preds, targets))\n        return loss\n\n    def validation_step(self, batch):\n        videos, targets = batch\n        videos = self.transforms(videos)\n        videos = videos.permute(0, 2, 1, 3, 4)\n        preds = self.model(videos)\n        loss = self.loss_fn(preds, targets.long())\n        self.log('val_loss', loss)\n        self.log('val_acc', self.accuracy(preds, targets))\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.model.parameters(), lr=self.lr)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:16:57.180143Z","iopub.execute_input":"2024-07-07T19:16:57.180515Z","iopub.status.idle":"2024-07-07T19:16:57.212743Z","shell.execute_reply.started":"2024-07-07T19:16:57.180486Z","shell.execute_reply":"2024-07-07T19:16:57.211882Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"test_video_1 = KeyDataset('video_1', \n                          labels_dir='/kaggle/input/keystroke/labels',\n                           videos_dir=\"/kaggle/input/keystroke/raw_frames_320\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:17:03.421965Z","iopub.execute_input":"2024-07-07T19:17:03.422656Z","iopub.status.idle":"2024-07-07T19:17:03.484024Z","shell.execute_reply.started":"2024-07-07T19:17:03.422624Z","shell.execute_reply":"2024-07-07T19:17:03.483155Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"dm = KeyDataModule(batch_size=8, \n                   labels_dir='/kaggle/input/keystroke/labels',\n                   videos_dir=\"/kaggle/input/keystroke/raw_frames_320\",\n                   train_vids=[\n                       'video_1', 'video_2', 'video_3', 'video_4', 'video_5', \n                        'video_6', 'video_7', 'video_8', 'video_9', 'video_10',\n                       'video_11', 'video_12', 'video_13', 'video_14', 'video_15', \n                       'video_16', 'video_17', 'video_18', 'video_19',\n                       'video_21', 'video_22', 'video_23', 'video_24', 'video_25', \n                       'video_26', 'video_27', 'video_28', 'video_29', 'video_30'], \n                   val_vids=['video_31', 'video_32', 'video_33'], \n                   test_vids=['video_34','video_35', 'video_36'])\nmodel = KeyClf(img_size=320, num_classes=len(id2Label), learning_rate=0.001, weights=dm.train_weights)\ntrainer = L.Trainer(\n    # deterministic=True,\n    devices=[0, 1],\n    accelerator=\"gpu\",\n    fast_dev_run=False,\n    max_time=\"00:11:00:00\",\n    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n)\ntrainer.fit(model, dm)\ntrainer.test(model, dm)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T19:17:05.929606Z","iopub.execute_input":"2024-07-07T19:17:05.930020Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Train: 77589; Val: 7168; Test: 4377\nTrain counts: [29360.  4182.   532.  7038.   376.  2938.   590.  1222.  1179.  4348.\n   682.   734.  1228.  2825.   407.   363.  1573.   850.  2352.  2539.\n   917.   346.  2248.  1843.  2776.  1417.   405.   587.   497.   779.\n   456.]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_887/4267700438.py:121: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\nINFO: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\nINFO: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\nINFO: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 2 processes\n----------------------------------------------------------------------------------------------------\n\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name       | Type               | Params | Mode \n----------------------------------------------------------\n0 | model      | ResNet             | 63.5 M | train\n1 | loss_fn    | CrossEntropyLoss   | 0      | train\n2 | accuracy   | MulticlassAccuracy | 0      | train\n3 | transforms | Compose            | 0      | train\n----------------------------------------------------------\n63.5 M    Trainable params\n0         Non-trainable params\n63.5 M    Total params\n254.118   Total estimated model params size (MB)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"# device = torch.device('cuda')\n# trained_model = KeyClf.load_from_checkpoint(\"/kaggle/input/keyclf/pytorch/v1/1/epoch7-step34979.ckpt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# video = 'video_36'\n# images = glob.glob(f'/kaggle/input/keycls-test/test_data/raw_frames/{video}/*.jpg')\n# img = cv2.imread(os.path.join(input_dir, jpg_files[0]))\n# height, width, _ = img.shape\n\n# output_file = f'./{video}.mp4'\n# # Define the codec and create VideoWriter object\n# fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 format\n# out = cv2.VideoWriter(output_file, fourcc, framerate=25, (width, height))\n\n# # Iterate through JPG files and write to video\n# for i in range(len(images)):\n#     img = cv2.imread(''/kaggle/input/keycls-test/test_data/raw_frames/{video}/frame_{i}.jpg')\n#     out.write(img)\n\n# # Release VideoWriter and close all windows\n# out.release()\n# cv2.destroyAllWindows()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = []\n\n# is_gpu = torch.cuda.is_available()\n\n# if is_gpu: \n#     trained_model.to(device)\n\n# trained_model.freeze()\n# i = 0\n# recording = True\n# window = []\n\n# while recording:\n#     # Less than 8 flen(window)rames => continue to collect frames....\n#     if len(window) < total_window:\n#         image = torchvision.io.read_image(f\"/kaggle/input/keycls-test/test_data/raw_frames/{video}/frame_{i}.jpg\")\n#         window.append(image)\n#     if len(window) == total_window:\n#         frames = torch.stack(window)\n#         frames = trained_model.transforms(frames)\n#         frames = frames.permute(1, 0, 2, 3)\n        \n#         if is_gpu: frames.to(device)\n#         out = F.softmax(trained_model.model(frames.unsqueeze(0)))[0]\n#         _id = torch.argmax(out)\n#         label = id2Label[_id]\n#         print(f\"{i - total_window - 1};{label};{out[_id]}\")\n    \n#         image = torchvision.io.read_image(f\"/kaggle/input/keycls-test/test_data/raw_frames/{video}/frame_{i}.jpg\")\n#         window.append(image)\n#         window = window[1:]\n    \n#     i += 1\n#     if i == len(images):\n#         recording = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}