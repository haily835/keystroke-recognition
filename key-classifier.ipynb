{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightning torchvision","metadata":{"execution":{"iopub.status.busy":"2024-07-05T18:26:32.157617Z","iopub.execute_input":"2024-07-05T18:26:32.158012Z","iopub.status.idle":"2024-07-05T18:26:45.352683Z","shell.execute_reply.started":"2024-07-05T18:26:32.157980Z","shell.execute_reply":"2024-07-05T18:26:45.351444Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: lightning in /opt/conda/lib/python3.10/site-packages (2.3.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2+cpu)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.3.1)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.2)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2+cpu)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.4.0.post0)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.4)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (3.1.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=2.0.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=2.0.0->lightning) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport lightning as L\nimport pandas as pd\nimport torchvision\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\nimport pathlib\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.io import read_video\nimport lightning as L\nfrom lightning.pytorch.loggers import CSVLogger\nimport torchmetrics\nfrom lightning.pytorch.callbacks import EarlyStopping\nfrom sklearn.metrics import classification_report, confusion_matrix, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom torchvision.transforms import CenterCrop, v2\nfrom datetime import datetime\nimport csv\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T19:48:06.563458Z","iopub.execute_input":"2024-07-05T19:48:06.564572Z","iopub.status.idle":"2024-07-05T19:48:06.572027Z","shell.execute_reply.started":"2024-07-05T19:48:06.564532Z","shell.execute_reply":"2024-07-05T19:48:06.570792Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"id2Label = ['[i]', 'BackSpace', ',', '[s]', '.', \n            'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n            'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', \n            'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n            'y', 'z']\nlabel2Id  = {label: i for i, label in enumerate(id2Label)}\nlabels_dir = './labels'\nvideos_dir = './videos'\n\nNUM_WORKERS = 2","metadata":{"execution":{"iopub.status.busy":"2024-07-05T18:58:27.837390Z","iopub.execute_input":"2024-07-05T18:58:27.837797Z","iopub.status.idle":"2024-07-05T18:58:27.845051Z","shell.execute_reply.started":"2024-07-05T18:58:27.837763Z","shell.execute_reply":"2024-07-05T18:58:27.843684Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Resnet","metadata":{}},{"cell_type":"code","source":"#### RESNET 3D #### \ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4) ).zero_()\n    \n    if isinstance(out.data, torch.cuda.FloatStorage): zero_pads = zero_pads.cuda()\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n    return out\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n\n        self.conv2 = nn.Conv3d(\n            planes, planes, \n            kernel_size=3, stride=stride, padding=1, \n            bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        \n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * 4)\n        \n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, sample_size, sample_duration, shortcut_type='B', num_classes=400):\n        \"\"\"\n        block: basic block or bottle neck\n        layers: define Resnet architecture 34, 101, 152 etc\n        sample size: image size\n        shortcut_type: 'A' or 'B'\n        num_classes: ...\n        \"\"\"\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n        self.layer2 = self._make_layer(block, 128, layers[1], shortcut_type, stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], shortcut_type, stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], shortcut_type, stride=2)\n        \n        \n        last_duration = int(math.ceil(sample_duration / 16))\n        last_size = int(math.ceil(sample_size / 32))\n        self.avgpool = nn.AvgPool3d(\n            (last_duration, last_size, last_size), stride=1)\n        \n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == 'A':\n                downsample = partial(\n                    downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(self.inplanes,planes * block.expansion,kernel_size=1,stride=stride,bias=False), \n                    nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\ndef resnet10(**kwargs): return ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\ndef resnet18(**kwargs): return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\ndef resnet34(**kwargs): return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\ndef resnet50(**kwargs): return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\ndef resnet101(**kwargs): return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\ndef resnet152(**kwargs): return ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\ndef resnet200(**kwargs): return ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T19:30:46.153164Z","iopub.execute_input":"2024-07-05T19:30:46.153591Z","iopub.status.idle":"2024-07-05T19:30:46.186738Z","shell.execute_reply.started":"2024-07-05T19:30:46.153560Z","shell.execute_reply":"2024-07-05T19:30:46.185364Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Lightning dataset","metadata":{}},{"cell_type":"code","source":"class KeyDataset(torch.utils.data.Dataset):\n    def __init__(self, video_name):\n        segments = []\n        # Infer idle frames.\n        df = pd.read_csv(f'{labels_dir}/{video_name}.csv')\n        for index, row in df.iterrows():\n            key_frame = int(row['Frame'])  # Frame number where key was pressed\n            key_value = row['Key']  # Key pressed\n            if key_value not in id2Label:\n                key_value = '[s]'\n            \n            if index == 0:\n                pos_start = max(key_frame - 3, 0)\n                pos_end = key_frame + 4\n                neg_start = 0\n                neg_end = pos_start - 1\n            else:\n                prev_key_frame = df.iloc[index - 1]['Frame']\n                pos_start = max(key_frame - 3, 0)\n                pos_end = key_frame + 4\n                prev_pos_window_end = prev_key_frame + 4\n                if (pos_start - prev_pos_window_end) - 1 >= 8:\n                    neg_start =  prev_pos_window_end + 1\n                    neg_end = pos_start - 1\n            # Negative class video segments before\n            j = neg_end \n            while j - 7 >= neg_start:\n                segments.append(([j - 7, j],\"[i]\"))\n                j -= 8\n            # Current video with keystroke\n            segments.append(([pos_start, pos_end], key_value))\n        \n        self.video_name = video_name\n        self.segments = segments\n    def __len__(self):\n        return len(self.segments)\n\n    def __getitem__(self, idx):\n        (start, end), label = self.segments[i]\n        \n        frames = []\n        for i in range(start, end + 1):\n            image = torchvision.io.read_image(f\"{videos_dir}/{self.video_name}/frame_{i}.jpg\")\n            \n            frames.append(image)\n       \n        return torch.stack(frames), label\n    \n    def get_class_counts(self):\n        labels = [segment[1] for segment in self.segments]\n        unique_elements, counts = np.unique(labels, return_counts=True)\n        occurrences = dict(zip(unique_elements, counts))\n        weights = np.zeros(len(id2Label))\n        for label, count in occurrences.items():\n            weights[label2Id[label]] = count\n        return weights\n\nclass KeyDataModule(L.LightningDataModule):\n    def __init__(self, batch_size, train_vids, val_vids, test_vids):\n        super().__init__()\n        self.batch_size = batch_size\n        self.train_datasets = [KeyDataset(video_name) for video_name in train_vids]\n        self.val_dataset = [KeyDataset(video_name) for video_name in val_vids]\n        self.test_datasets = [KeyDataset(video_name) for video_name in test_vids]\n        \n        self.train_dataset = torch.utils.data.ConcatDataset(self.train_datasets)\n        self.test_dataset = torch.utils.data.ConcatDataset(self.test_datasets)\n        self.val_dataset = torch.utils.data.ConcatDataset(self.val_datasets)\n        \n        print(f\"\"\"Train: {len(self.train_dataset)}; \n                Val: {len(self.val_dataset)};\n                Test: {len(self.test_dataset)}\n        \"\"\")\n        \n        train_counts = np.array([d.get_class_counts() for d in train_datasets]).sum(axis=1)\n        print(f\"Train counts: {train_counts}\")\n        train_total_samples = np.array([len(d) for d in train_datasets]).sum(axis=0)\n        train_weights = train_counts / (train_total_samples * len(id2Label))\n                                        \n    def train_dataloader(self):\n        return DataLoader(self.train_datasets, \n                          batch_size=self.batch_size, \n                          num_workers=NUM_WORKERS, persistent_workers=True)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_datasets, \n                          batch_size=self.batch_size, \n                          num_workers=NUM_WORKERS, persistent_workers=True,\n                          shuffle=False)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_datasets, \n                          batch_size=self.batch_size, \n                          num_workers=NUM_WORKERS, persistent_workers=True, \n                          shuffle=False)\n\nclass KeyClf(L.LightningModule):\n    def __init__(self, img_size, num_classes, learning_rate, weights):\n        super().__init__()\n        self.model = resnet101(sample_size=img_size, \n                               sample_duration=8,\n                               shortcut_type='B', \n                               num_classes=num_classes)\n        \n        self.loss_fn = torch.nn.CrossEntropyLoss(torch.tensor(weights))\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.lr = learning_rate\n        self.transforms = v2.Compose([\n            v2.CenterCrop(320),\n            v2.ToDtype(torch.float32, scale=True),\n        ])\n        \n        self.test_preds = []\n        self.test_targets = []\n        self.save_hyperparameters()\n\n\n    def test_step(self, batch):\n        videos, targets = batch\n        videos = self.transforms(videos)\n        preds = self.model(videos)\n        pred_ids = torch.argmax(self.model(videos), dim=1).squeeze()\n        pred_labels = [id2Label[_id] for _id in pred_ids]\n        self.test_preds += pred_labels\n        self.test_targets += [id2Label[_id] for _id in targets]\n        \n        loss = self.loss_fn(preds, targets.long())\n        self.log_dict({'test_acc': self.accuracy(preds, targets), 'test_loss': loss})\n    \n    def on_test_end(self):\n        print(classification_report(self.test_targets, self.test_preds))\n        \n    def training_step(self, batch):\n        videos, targets = batch\n        videos = self.transforms(videos)\n        preds = self.model(videos)\n        loss = self.loss_fn(preds, targets.long())\n        self.log_dict({\"train_loss\": loss, \"train_acc\": self.accuracy(preds, targets)})\n        return loss\n\n    def validation_step(self, batch):\n        videos, targets = batch\n        videos = self.transforms(videos)\n        preds = self.model(videos)\n        loss = self.loss_fn(preds, targets.long())\n        self.log_dict({\"val_loss\": loss, \"val_acc\": self.accuracy(preds, targets)})\n        return loss\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.model.parameters(), lr=self.lr)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T20:01:35.778978Z","iopub.execute_input":"2024-07-05T20:01:35.779396Z","iopub.status.idle":"2024-07-05T20:01:35.806084Z","shell.execute_reply.started":"2024-07-05T20:01:35.779364Z","shell.execute_reply":"2024-07-05T20:01:35.804803Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-07-05T19:52:48.433005Z","iopub.execute_input":"2024-07-05T19:52:48.434106Z","iopub.status.idle":"2024-07-05T19:52:48.441641Z","shell.execute_reply.started":"2024-07-05T19:52:48.434068Z","shell.execute_reply":"2024-07-05T19:52:48.440492Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]}]}