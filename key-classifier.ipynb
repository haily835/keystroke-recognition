{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-07-05T18:26:32.158012Z","iopub.status.busy":"2024-07-05T18:26:32.157617Z","iopub.status.idle":"2024-07-05T18:26:45.352683Z","shell.execute_reply":"2024-07-05T18:26:45.351444Z","shell.execute_reply.started":"2024-07-05T18:26:32.157980Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: lightning in /opt/conda/lib/python3.10/site-packages (2.3.2)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2+cpu)\n","Requirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\n","Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.3.1)\n","Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.2)\n","Requirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\n","Requirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\n","Requirement already satisfied: torch<4.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2+cpu)\n","Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.4.0.post0)\n","Requirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.4)\n","Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\n","Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.5)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (3.13.1)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.0.0->lightning) (3.1.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.1)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (69.0.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=2.0.0->lightning) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=2.0.0->lightning) (1.3.0)\n"]}],"source":["!pip install lightning torchvision"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T19:48:06.564572Z","iopub.status.busy":"2024-07-05T19:48:06.563458Z","iopub.status.idle":"2024-07-05T19:48:06.572027Z","shell.execute_reply":"2024-07-05T19:48:06.570792Z","shell.execute_reply.started":"2024-07-05T19:48:06.564532Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import lightning as L\n","import pandas as pd\n","import torchvision\n","import numpy as np\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import math\n","from functools import partial\n","import pathlib\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.io import read_video\n","import lightning as L\n","from lightning.pytorch.loggers import CSVLogger\n","import torchmetrics\n","from lightning.pytorch.callbacks import EarlyStopping\n","from sklearn.metrics import classification_report, confusion_matrix, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from torchvision.transforms import CenterCrop, v2\n","from datetime import datetime\n","import csv\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T18:58:27.837797Z","iopub.status.busy":"2024-07-05T18:58:27.837390Z","iopub.status.idle":"2024-07-05T18:58:27.845051Z","shell.execute_reply":"2024-07-05T18:58:27.843684Z","shell.execute_reply.started":"2024-07-05T18:58:27.837763Z"},"trusted":true},"outputs":[],"source":["id2Label = ['[i]', 'BackSpace', ',', '[s]', '.', \n","            'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n","            'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', \n","            'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n","            'y', 'z']\n","label2Id  = {label: i for i, label in enumerate(id2Label)}\n","labels_dir = './labels'\n","videos_dir = './datasets/raw_frames'\n","\n","NUM_WORKERS = 0"]},{"cell_type":"markdown","metadata":{},"source":["# Resnet"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T19:30:46.153591Z","iopub.status.busy":"2024-07-05T19:30:46.153164Z","iopub.status.idle":"2024-07-05T19:30:46.186738Z","shell.execute_reply":"2024-07-05T19:30:46.185364Z","shell.execute_reply.started":"2024-07-05T19:30:46.153560Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#### RESNET 3D #### \n","def conv3x3x3(in_planes, out_planes, stride=1):\n","    # 3x3x3 convolution with padding\n","    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","def downsample_basic_block(x, planes, stride):\n","    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n","    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4) ).zero_()\n","    \n","    if isinstance(out.data, torch.cuda.FloatStorage): zero_pads = zero_pads.cuda()\n","    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n","    return out\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm3d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm3d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm3d(planes)\n","\n","        self.conv2 = nn.Conv3d(\n","            planes, planes, \n","            kernel_size=3, stride=stride, padding=1, \n","            bias=False)\n","        self.bn2 = nn.BatchNorm3d(planes)\n","        \n","        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm3d(planes * 4)\n","        \n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, layers, sample_size, sample_duration, shortcut_type='B', num_classes=400):\n","        \"\"\"\n","        block: basic block or bottle neck\n","        layers: define Resnet architecture 34, 101, 152 etc\n","        sample size: image size\n","        shortcut_type: 'A' or 'B'\n","        num_classes: ...\n","        \"\"\"\n","        self.inplanes = 64\n","        super(ResNet, self).__init__()\n","        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n","        self.bn1 = nn.BatchNorm3d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n","        self.layer2 = self._make_layer(block, 128, layers[1], shortcut_type, stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], shortcut_type, stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], shortcut_type, stride=2)\n","        \n","        \n","        last_duration = int(math.ceil(sample_duration / 16))\n","        last_size = int(math.ceil(sample_size / 32))\n","        self.avgpool = nn.AvgPool3d(\n","            (last_duration, last_size, last_size), stride=1)\n","        \n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n","            elif isinstance(m, nn.BatchNorm3d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            if shortcut_type == 'A':\n","                downsample = partial(\n","                    downsample_basic_block,\n","                    planes=planes * block.expansion,\n","                    stride=stride)\n","            else:\n","                downsample = nn.Sequential(\n","                    nn.Conv3d(self.inplanes,planes * block.expansion,kernel_size=1,stride=stride,bias=False), \n","                    nn.BatchNorm3d(planes * block.expansion))\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","def resnet10(**kwargs): return ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\n","def resnet18(**kwargs): return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n","def resnet34(**kwargs): return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n","def resnet50(**kwargs): return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n","def resnet101(**kwargs): return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n","def resnet152(**kwargs): return ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n","def resnet200(**kwargs): return ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)"]},{"cell_type":"markdown","metadata":{},"source":["# Lightning dataset"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T20:01:35.779396Z","iopub.status.busy":"2024-07-05T20:01:35.778978Z","iopub.status.idle":"2024-07-05T20:01:35.806084Z","shell.execute_reply":"2024-07-05T20:01:35.804803Z","shell.execute_reply.started":"2024-07-05T20:01:35.779364Z"},"trusted":true},"outputs":[],"source":["class KeyDataset(torch.utils.data.Dataset):\n","    def __init__(self, video_name):\n","        segments = []\n","        # Infer idle frames.\n","        df = pd.read_csv(f'{labels_dir}/{video_name}.csv')\n","        for index, row in df.iterrows():\n","            key_frame = int(row['Frame'])  # Frame number where key was pressed\n","            key_value = row['Key']  # Key pressed\n","            if key_value not in id2Label:\n","                key_value = '[s]'\n","            \n","            if index == 0:\n","                pos_start = max(key_frame - 3, 0)\n","                pos_end = key_frame + 4\n","                neg_start = 0\n","                neg_end = pos_start - 1\n","            else:\n","                prev_key_frame = df.iloc[index - 1]['Frame']\n","                pos_start = max(key_frame - 3, 0)\n","                pos_end = key_frame + 4\n","                prev_pos_window_end = prev_key_frame + 4\n","                if (pos_start - prev_pos_window_end) - 1 >= 8:\n","                    neg_start =  prev_pos_window_end + 1\n","                    neg_end = pos_start - 1\n","            # Negative class video segments before\n","            j = neg_end \n","            while j - 7 >= neg_start:\n","                segments.append(([j - 7, j],\"[i]\"))\n","                j -= 8\n","            # Current video with keystroke\n","            segments.append(([pos_start, pos_end], key_value))\n","        \n","        self.video_name = video_name\n","        self.segments = segments\n","    def __len__(self):\n","        return len(self.segments)\n","\n","    def __getitem__(self, idx):\n","        (start, end), label = self.segments[idx]\n","        \n","        frames = []\n","        for i in range(start, end + 1):\n","            image = torchvision.io.read_image(f\"{videos_dir}/{self.video_name}/frame_{i}.jpg\")\n","            \n","            frames.append(image)\n","       \n","        return torch.stack(frames), label2Id[label]\n","    \n","    def get_class_counts(self):\n","        labels = [segment[1] for segment in self.segments]\n","        unique_elements, counts = np.unique(labels, return_counts=True)\n","        occurrences = dict(zip(unique_elements, counts))\n","        weights = np.zeros(len(id2Label))\n","        for label, count in occurrences.items():\n","            weights[label2Id[label]] = count\n","        return weights\n","\n","class KeyDataModule(L.LightningDataModule):\n","    def __init__(self, batch_size, train_vids, val_vids, test_vids):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        self.train_datasets = [KeyDataset(video_name) for video_name in train_vids]\n","        self.val_datasets = [KeyDataset(video_name) for video_name in val_vids]\n","        self.test_datasets = [KeyDataset(video_name) for video_name in test_vids]\n","        \n","        self.train_dataset = torch.utils.data.ConcatDataset(self.train_datasets)\n","        self.test_dataset = torch.utils.data.ConcatDataset(self.test_datasets)\n","        self.val_dataset = torch.utils.data.ConcatDataset(self.val_datasets)\n","        \n","        print(f\"Train: {len(self.train_dataset)}; Val: {len(self.val_dataset)}; Test: {len(self.test_dataset)}\")\n","        \n","        train_counts = np.array(\n","            [d.get_class_counts() for d in self.train_datasets]).sum(axis=0)\n","        print(f\"Train counts: {train_counts}\")\n","        train_total_samples = np.array([len(d) for d in self.train_datasets]).sum(axis=0)\n","        self.train_weights = train_counts / (train_total_samples * len(id2Label))\n","                                        \n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, \n","                          batch_size=self.batch_size, \n","                          num_workers=NUM_WORKERS)\n","    \n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, \n","                          batch_size=self.batch_size, \n","                          num_workers=NUM_WORKERS,\n","                          shuffle=False)\n","    \n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, \n","                          batch_size=self.batch_size, \n","                          num_workers=NUM_WORKERS,\n","                          shuffle=False)\n","\n","class KeyClf(L.LightningModule):\n","    def __init__(self, img_size, num_classes, learning_rate, weights):\n","        super().__init__()\n","        self.model = resnet101(sample_size=img_size, \n","                               sample_duration=8,\n","                               shortcut_type='B', \n","                               num_classes=num_classes)\n","        \n","        self.loss_fn = torch.nn.CrossEntropyLoss(torch.tensor(weights).float())\n","        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n","        self.lr = learning_rate\n","        self.transforms = v2.Compose([\n","            v2.CenterCrop(img_size),\n","            v2.ToDtype(torch.float32, scale=True),\n","        ])\n","        \n","        self.test_preds = []\n","        self.test_targets = []\n","        self.save_hyperparameters()\n","\n","\n","    def test_step(self, batch):\n","        videos, targets = batch\n","        videos = self.transforms(videos)\n","        videos = videos.permute(0, 2, 1, 3, 4)\n","        preds = self.model(videos)\n","\n","        pred_ids = torch.argmax(self.model(videos), dim=1).squeeze()\n","        pred_labels = [id2Label[_id] for _id in pred_ids]\n","        self.test_preds += pred_labels\n","        self.test_targets += [id2Label[_id] for _id in targets]\n","        \n","        loss = self.loss_fn(preds, targets.long())\n","        self.log_dict({'test_acc': self.accuracy(preds, targets), 'test_loss': loss})\n","    \n","    def on_test_end(self):\n","        print(classification_report(self.test_targets, self.test_preds))\n","        \n","    def training_step(self, batch):\n","        videos, targets = batch\n","        videos = self.transforms(videos)\n","        videos = videos.permute(0, 2, 1, 3, 4)\n","        preds = self.model(videos)\n","        loss = self.loss_fn(preds, targets.long())\n","        self.log_dict({\"train_loss\": loss, \"train_acc\": self.accuracy(preds, targets)})\n","        return loss\n","\n","    def validation_step(self, batch):\n","        videos, targets = batch\n","        videos = self.transforms(videos)\n","        videos = videos.permute(0, 2, 1, 3, 4)\n","        preds = self.model(videos)\n","        loss = self.loss_fn(preds, targets.long())\n","        self.log_dict({\"val_loss\": loss, \"val_acc\": self.accuracy(preds, targets)})\n","        return loss\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.model.parameters(), lr=self.lr)"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-07-05T19:52:48.434106Z","iopub.status.busy":"2024-07-05T19:52:48.433005Z","iopub.status.idle":"2024-07-05T19:52:48.441641Z","shell.execute_reply":"2024-07-05T19:52:48.440492Z","shell.execute_reply.started":"2024-07-05T19:52:48.434068Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 5470; Val: 3578; Test: 3625\n","Train counts: [4.078e+03 1.140e+02 1.100e+01 2.090e+02 1.600e+01 8.200e+01 1.100e+01\n"," 5.400e+01 3.200e+01 1.420e+02 2.000e+01 3.200e+01 2.700e+01 8.700e+01\n"," 2.000e+00 1.000e+01 4.800e+01 2.200e+01 8.000e+01 7.000e+01 1.500e+01\n"," 3.000e+00 8.700e+01 5.600e+01 8.400e+01 2.900e+01 1.100e+01 1.100e+01\n"," 4.000e+00 2.300e+01 0.000e+00]\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/9w/wb24vz290fx3x35b66j6pds80000gn/T/ipykernel_16207/4267700438.py:121: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n","  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n","GPU available: True (mps), used: False\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","/Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n","Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n","\n","  | Name       | Type               | Params | Mode \n","----------------------------------------------------------\n","0 | model      | ResNet             | 85.3 M | train\n","1 | loss_fn    | CrossEntropyLoss   | 0      | train\n","2 | accuracy   | MulticlassAccuracy | 0      | train\n","3 | transforms | Compose            | 0      | train\n","----------------------------------------------------------\n","85.3 M    Trainable params\n","0         Non-trainable params\n","85.3 M    Total params\n","341.235   Total estimated model params size (MB)\n","/Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n","/Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] targets:  tensor([0, 0])\n","preds:  tensor([[ 0.2252,  0.3005,  0.4454,  0.4013,  0.3401,  0.0026,  0.1910,  0.4247,\n","         -0.7442,  0.1196, -0.3172, -0.0877, -0.3659, -0.7182, -0.4485,  0.0177,\n","         -0.6784, -0.3468, -0.5059, -0.1679,  0.7041,  0.3600,  0.5645, -0.2369,\n","          0.5347,  0.0423,  0.1157, -0.0775, -0.7890,  0.1155,  0.7408],\n","        [ 0.1340,  0.2711,  0.3904,  0.5865,  0.5353, -0.0374,  0.3785,  0.3999,\n","         -0.9755, -0.0054, -0.4410, -0.2041, -0.4371, -0.6696, -0.5009, -0.0200,\n","         -0.6782, -0.3288, -0.6365,  0.0731,  0.7066,  0.3880,  0.4506, -0.2134,\n","          0.6695, -0.0062,  0.2850, -0.0511, -0.7769,  0.0993,  0.8175]],\n","       grad_fn=<AddmmBackward0>)\n","Epoch 0: 100%|██████████| 1/1 [00:05<00:00,  0.17it/s]"]},{"name":"stderr","output_type":"stream","text":["`Trainer.fit` stopped: `max_steps=1` reached.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0: 100%|██████████| 1/1 [00:05<00:00,  0.17it/s]"]},{"name":"stderr","output_type":"stream","text":["/Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Testing DataLoader 0: 100%|██████████| 1/1 [00:02<00:00,  0.45it/s]\n","              precision    recall  f1-score   support\n","\n","         [i]       1.00      1.00      1.00         2\n","\n","    accuracy                           1.00         2\n","   macro avg       1.00      1.00      1.00         2\n","weighted avg       1.00      1.00      1.00         2\n","\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.587194561958313     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.587194561958313    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'test_acc': 1.0, 'test_loss': 1.587194561958313}]"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["dm = KeyDataModule(batch_size=2, \n","                   train_vids=['video_3'], \n","                   val_vids=['video_2'], \n","                   test_vids=['video_33'])\n","model = KeyClf(img_size=320,\n","                   num_classes=len(id2Label), \n","                   learning_rate=0.001, \n","                   weights=dm.train_weights)\n","trainer = L.Trainer(\n","    # deterministic=True,\n","    # devices=0,\n","    max_time=\"00:11:00:00\",\n","    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=10)],\n","    fast_dev_run=True,\n","    accelerator=\"cpu\")\n","trainer.fit(model, dm)\n","trainer.test(model, dm)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
